ðŸ“Œ DAY 2 CONTENT

âœ” Label Encoding
âœ” One Hot Encoding
âœ” Handling Missing Values
âœ” Trainâ€“Test Split
âœ” Evaluation Metrics (MSE, RMSE, MAE, RÂ²)
âœ” Multiple Linear Regression
âœ” Polynomial Regression
âœ” Feature Scaling (Standardization + Normalization)

-----------------------------------------
ðŸ“˜ 1. Encoding Techniques
-----------------------------------------
Why Encoding?

ML models only understand numbers, not text.

â‘  Label Encoding (Ordinal Categories)

Use when categories have rank/priority/order
(e.g., Low < Medium < High)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Education'] = le.fit_transform(df['Education'])

â‘¡ One Hot Encoding (Nominal Categories)

Use when categories do not have order

df = pd.get_dummies(df, columns=['Color'], drop_first=True)


drop_first=True â†’ avoids dummy variable trap.

-----------------------------------------
ðŸ“˜ 2. Handling Missing Values
-----------------------------------------
Mean â†’ Numerical + Normal Distributed
Median â†’ Numerical + Skewed (outliers)
Mode â†’ Categorical
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Income'].fillna(df['Income'].median(), inplace=True)
df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)

-----------------------------------------
ðŸ“˜ 3. Trainâ€“Test Split
-----------------------------------------

Purpose: Evaluate model performance on unseen data.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

-----------------------------------------
ðŸ“˜ 4. Model Evaluation Metrics
-----------------------------------------
Mean Squared Error (MSE)
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)

Root Mean Squared Error (RMSE)
rmse = mse**0.5


(Easiest to interpret)

Mean Absolute Error (MAE)
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)

RÂ² Score
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)


Higher RÂ² â†’ better model.

-----------------------------------------
ðŸ“˜ 5. Multiple Linear Regression (MLR)
-----------------------------------------

Used when you have multiple independent features.

ð‘¦
=
ð‘
0
+
ð‘
1
ð‘¥
1
+
ð‘
2
ð‘¥
2
+
.
.
.
+
ð‘
ð‘›
ð‘¥
ð‘›
y=b
0
	â€‹

+b
1
	â€‹

x
1
	â€‹

+b
2
	â€‹

x
2
	â€‹

+...+b
n
	â€‹

x
n
	â€‹

Code:
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

-----------------------------------------
ðŸ“˜ 6. Polynomial Regression
-----------------------------------------

Used when data pattern is curved (non-linear).

ð‘¦
=
ð‘
0
+
ð‘
1
ð‘¥
+
ð‘
2
ð‘¥
2
+
ð‘
3
ð‘¥
3
.
.
.
y=b
0
	â€‹

+b
1
	â€‹

x+b
2
	â€‹

x
2
+b
3
	â€‹

x
3
...
Code:
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

model = LinearRegression()
model.fit(X_poly, y)

-----------------------------------------
ðŸ“˜ 7. Feature Scaling
-----------------------------------------
Why Scaling?

Some algorithms depend on distance, like:

Logistic Regression (GD)

KNN

SVM

Neural Networks

â‘  Standardization (Z-Score Scaling)

Centers data around mean = 0, std = 1

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

â‘¡ Normalization (MinMax Scaling)

Scales values between 0 and 1

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

-----------------------------------------
ðŸ“Œ DAY 2 SUMMARY TABLE
-----------------------------------------
Topic	Why Use	Extra Notes
Label Encoding	Ordered categories	Converts text â†’ integers
One Hot Encoding	Non-ordered categories	Avoid dummy trap
Missing Values	Clean dataset	Mean/Median/Mode
Trainâ€“Test Split	Validation	test_size = 0.2
MSE / RMSE	Error	RMSE easiest
MAE	Outlier-safe error	Absolute difference
RÂ² Score	Model performance	0â€“1 range
MLR	Multi-feature regression	Linear
Polynomial Regression	Non-linear	degree=2,3
Standardization	GD, KNN, SVM	mean=0
MinMax	NN, bounded	0â€“1
