Day 3 â€“ Decision Tree Classifier  
â€¢ Notes: Entropy, Gini, Information Gain  
â€¢ Implemented DecisionTreeClassifier with pruning  
â€¢ Added confusion matrix + accuracy + tree visualization  


ğŸ“Œ DAY 3 â€” Decision Tree Classifier
1ï¸âƒ£ What is a Decision Tree?

A supervised ML algorithm used for:
âœ” Classification
âœ” Regression

It splits data using questions until samples become pure.

Example:

Is CreditScore < 650?
â”œâ”€â”€ Yes â†’ Default
â””â”€â”€ No  â†’ Safe

2ï¸âƒ£ Key Concepts
ğŸŸ¤ Node Types

Root Node: first split

Decision Node: internal splits

Leaf Node: final class

3ï¸âƒ£ Purity Measures
ğŸ”¸ Entropy (ID3 Algorithm)

Measures impurity.


Entropy=âˆ’p1â€‹log2â€‹p1â€‹âˆ’p2â€‹log2â€‹p2â€‹
	â€‹

â—¼ 0 = Pure
â—¼ High = Mixed classes

ğŸ”¸ Gini Index (CART Algorithm â€“ sklearn default)


Gini=1âˆ’(p12â€‹+p22â€‹)
	

âœ” Faster
âœ” sklearn uses CART + Gini by default


4ï¸âƒ£ Information Gain

IG=Entropy(parent)âˆ’âˆ‘nniâ€‹â€‹Entropy(childiâ€‹)

â†’ Higher IG = better split.


5ï¸âƒ£ Overfitting Problem

Decision Trees easily overfit due to deep branches.

âœ” High training accuracy
âœ˜ Poor test accuracy

6ï¸âƒ£ Pruning Techniques
Pre-Pruning

Limit tree before fully growing:

max_depth
min_samples_split
min_samples_leaf

Post-Pruning

Grow full tree then cut branches (cost complexity pruning):

ccp_alpha

ğŸ§ª PYTHON IMPLEMENTATION
