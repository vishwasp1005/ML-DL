ğŸŒ³ Random Forest â€” Ensemble Learning

Random Forest = Many Decision Trees trained on random subsets.

âœ” Why RF works better than a single decision tree?

Less overfitting

Less variance

More stable

Handles missing values

Works well on high-dimensional data

## ğŸŒŸ Bagging â€” Bootstrap Aggregation

RF uses Bagging:

Each tree gets random rows

Each tree gets random columns

All trees vote â†’ Final prediction

Prediction=MajorityVote(trees)


## ğŸ¯ Feature Importance

RF gives the importance of each feature:

Higher value = More effect on prediction.

Used a lot in ML interviews.


# ğŸŸ¦ KNN â€” K-Nearest Neighbors Classifier

Non-parametric algorithm
â†’ NO training phase
â†’ Prediction happens by finding nearest neighbors.

## ğŸ“Œ Distance Metrics
1. Euclidean Distance (Default)

d=(x1â€‹âˆ’y1â€‹)2+(x2â€‹âˆ’y2â€‹)2+...

â€‹
2. Manhattan Distance

d=âˆ£x1â€‹âˆ’y1â€‹âˆ£+âˆ£x2â€‹âˆ’y2â€‹âˆ£

## ğŸ¯ Scaling Required

KNN uses distance â†’ scale your data.

Use StandardScaler or MinMaxScaler.

## ğŸ” Choosing K value

Low K â†’ Overfitting

High K â†’ Underfitting

Best K â†’ odd numbers (5,7,9)

Use Elbow method


