DAY 6 â€” MODEL EVALUATION & HYPERPARAMETER TUNING
ğŸŸ§ 1ï¸âƒ£ CONFUSION MATRIX (Base of Everything)


                  predicted 0     predicted 1 
				  
	actual 0         TN               FN

	actual 1         FP               TP


TP â†’ Correct positive

TN â†’ Correct negative

FP â†’ False alarm

FN â†’ Missed positive

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

ğŸŸª 2ï¸âƒ£ PRECISION, RECALL & F1 SCORE
ğŸ”¹ Precision

Out of predicted positives, how many are correct?


Precision=TP+FPTPâ€‹
	â€‹


Use when false positives are costly (spam detection).

ğŸ”¹ Recall

Out of actual positives, how many did we catch?


Recall=TP+FNTPâ€‹â€‹


Use when false negatives are costly (disease detection).

ğŸ”¹ F1 Score

Balance between Precision & Recall


F1=   2Ã—PrecisionÃ—Recall / Precision+Recall
	â€‹

ğŸ”µ 3ï¸âƒ£ ROC CURVE & AUC SCORE

ROC Curve â†’ TPR vs FPR

AUC â†’ Area under ROC curve

AUC Score	Model Quality


0.5	Random

0.7â€“0.8	Good

0.8â€“0.9	Very Good

>0.9	Excellent


ğŸŸ© 4ï¸âƒ£ CROSS VALIDATION (Very Important)

Instead of single train-test split, we train model multiple times.


âœ” More reliable accuracy
âœ” Reduces overfitting


ğŸŸ¨ 5ï¸âƒ£ GRIDSEARCH CV (Hyperparameter Tuning)

Automatically finds best parameters.

âœ” Improves performance

